defaults:
  - _self_
  - model: distilbert
  - tuning: dative_DO_give_active
  - override hydra/job_logging: tuner_log_conf


dev: [] # which data to use as a dev set. This should be the name(s) of a tuning file. Leave blank to not use a dev set. It should be passed as a list
# note that any tokens to mask must be identical in tuning and dev sets. you can also use "best_matches" to use all datasets that differ from the tuning data in a single way as dev sets
dev_exclude: [] # you can specify exclusion criteria for dev sets as a list of strings when using "best_matches" for dev
n: 0 # How many models to fit? Only used when --multirun/-m is set. Specified as 'n=range(0,i)'

hyperparameters:
  lr: 0.001
  max_epochs: 70 # how many epochs should the model be allowed to tune for
  min_epochs: ${hyperparameters.epochs} # how many epochs must the model tune for?
  masked_tuning_style: 'always' # use the original style of bert tuning, roberta-style tuning, always used masked version, or don't mask
  strip_punct: false # Strip punctuation from sentences (true), or leave it intact (false)
  patience: ${hyperparameters.epochs} # how many epochs to continue training without improving mean loss on the dev set(s)?
  delta: 0 # how much improvement counts as enough to continue? improvement on mean dev loss must be greater than this to reset the patience counter
  
# Masked tuning style values are 'always,' 'bert,' 'roberta', or 'none'
# 'always' always masks tokens
# 'bert' masks tokens using the original bert method,
# where 80% of the time the tokens are masked,
# 10% of the time they are replaced by the original token,
# and 10% of the time they are replaced by a random word from the vocubulary
# it does this just once before fine-tuning
# for 'roberta' style tuning, it does the bert style produces but rerolls every epoch
# 'none' means no masking
  
hydra:
  run: 
    dir: './outputs/${tuning.name}/${model.friendly_name}-${maskname:${hyperparameters.masked_tuning_style}}-${spname:${hyperparameters.strip_punct}}/${now:%Y-%m-%d_%H-%M-%S}'
  sweep:
    dir: './outputs'
    subdir: '${tuning.name}/${model.friendly_name}-${maskname:${hyperparameters.masked_tuning_style}}-${spname:${hyperparameters.strip_punct}}/${now:%Y-%m-%d_%H-%M-%S}-${n}'