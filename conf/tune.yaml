defaults:
  - _self_
  - model: distilbert
  - tuning: dative_DO_give_active
  - override hydra/job_logging: tuner_log_conf

dev: [] # which data to use as a dev set. This should be the name(s) of a tuning file. Leave blank to not use a dev set. It should be passed as a list
# note that any tokens to mask must be identical in tuning and dev sets. you can also use "best_matches" to use all datasets that differ from the tuning data in a single way as dev sets
dev_exclude: [] # you can specify exclusion criteria for dev sets as a list of strings when using "best_matches" for dev
n: 0 # How many models to fit? Only used when --multirun/-m is set. Specified as 'n=range(0,i)'

hyperparameters:
  lr: 0.001
  max_epochs: 70 # how many epochs should the model be allowed to tune for
  min_epochs: ${hyperparameters.max_epochs} # how many epochs must the model tune for?
  patience: ${hyperparameters.max_epochs} # how many epochs to continue training without improving mean loss on the dev set(s)?
  delta: 0 # how much improvement counts as enough to continue? improvement on mean dev loss must be greater than this to reset the patience counter
  masked_tuning_style: always # use the original style of bert tuning, roberta-style tuning, always used masked version, or don't mask
  strip_punct: false # Strip punctuation from sentences (true), or leave it intact (false)
  unfreezing: none # what kind of unfreezing to use. 
                   # possible options are 'gradual{int}', 
                   #   where {int} specifies the number of epochs to go between unfreezing one layer and the next lowest layer (see https://arxiv.org/pdf/1801.06146.pdf)
                   # 'mixout{float}', where the float specifies the probability with which a parameter in the model being fine-tuned is replaced with the parameter from the pre-trained model
                   # 'complete', 'none', or an int specifying the highest layer that should remain frozen
  mask_args: false # whether to mask arguments in training data (only used for newverb experiments)
  use_kl_baseline_loss: false

kl_loss_params:
  dataset: datamaker/datasets/miniboki-2022-04-01_22-58-30/miniboki
  batch_size: 32
  n_examples_per_step: 100
  scaleby: 0.5
  masking: none
  
debug: false # whether to print predictions for sample sentences every epoch.
use_gpu: false # whether to use gpu support

# Masked tuning style values are 'always,' 'bert,' 'roberta', or 'none'
# 'always' always masks tokens
# 'bert' masks tokens using the original bert method,
# where 80% of the time the tokens are masked,
# 10% of the time they are replaced by the original token,
# and 10% of the time they are replaced by a random word from the vocubulary
# it does this just once before fine-tuning
# for 'roberta' style tuning, it does the bert style produces but rerolls every epoch
# 'none' means no masking
  
hydra:
  run: 
    dir: outputs/${get_dir_name:${model},${tuning},${hyperparameters},${kl_loss_params}}/${now:%Y-%m-%d_%H-%M-%S.%f}
  sweep:
    dir: outputs
    subdir: ${get_dir_name:${model},${tuning},${hyperparameters},${kl_loss_params}}/${now:%Y-%m-%d_%H-%M-%S.%f}-${n}
