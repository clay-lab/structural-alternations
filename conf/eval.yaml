defaults:
  - _self_
  - data: syn_give_give_ext
  - override hydra/job_logging: tuner_log_conf

criteria: all # to be evaluated, a model must contain these strings in the relative path to its checkpoint directory
              # 'all' means no exclusions

epoch: best_mean
# which epoch to evaluate the model on. None means the final epoch.
# 'best_sumsq' means the epoch meeting the following criteria:
# (1) it has the minimum loss for at least one set evaluated during training
# (2) if best_sumsq it minimizes the sumsq differences between the average loss
#     at that epoch compared to other epochs meeting criterion (1)
# 'best_mean' means the epoch that has lowest mean loss on dev sets

topk_mask_token_predictions: '' # how many of the top predictions to get for masked tokens in debug sentences?

k: 50 # get the k subword tokens with the most similar embeddings to the trained novel tokens
num_tsne_words: 500 # plot the tsne of the first num_tsne_words embeddings + the novel tokens

# if provided, and the model used unfreezing, compare the model's performance to a baseline on a dataset
comparison_dataset: datamaker/datasets/miniboki-2022-04-01_22-58-30/miniboki
comparison_n_exs: 100 # how many sentences from the comparison dataset to use? blank means all
                       # otherwise, pull this many random examples from the dataset to use
comparison_masking: none # how to mask sentences during evaluation

dir: '' # the directory containing the model checkpoint to evaluate
summarize: false # whether to create an overall summary when evaluating multiple models

debug: false # whether to print model predictions for some sample sentences. only used for newverb experiments
use_gpu: false # whether to use gpu for eval
rerun: false # whether to rerun eval on model checkpoints that already have the expected number of results files

hydra:
  run:
    dir: ./${dir}/eval-${dirname:${criteria},${data.name}}
  sweep:
    dir: ./${dir}
    subdir: eval-${dirname:${criteria},${data.name}}