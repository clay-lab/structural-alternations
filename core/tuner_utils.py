# tuner_utils.py
#
# utility functions for tuner.py
import os
import re
import json
import gzip
import torch
import torch.nn as nn
import random
import inspect
import logging
import itertools

import numpy as np
import pandas as pd

from math import sqrt
from tqdm import tqdm
from glob import glob
from copy import deepcopy
from typing import *
from shutil import copyfileobj
from datasets import Dataset, DatasetDict
from functools import partial
from omegaconf import OmegaConf, DictConfig, ListConfig
from scipy.stats import pearsonr
from transformers import AutoTokenizer

from datasets import load_dataset, Dataset
from datasets.utils import logging as dataset_utils_logging
from datasets.utils import disable_progress_bar
disable_progress_bar()
dataset_utils_logging.set_verbosity_error()

log = logging.getLogger(__name__)

# grammatical functions in order of prominence for sorting things
GF_ORDER = [
	'[subj]', 
	'[obj]', 
	'[2obj]', 
	'[iobj]', 
	'[obl]',
	'[pobj]', 
	'[adj]'
]

# short useful functions
def flatten(l: List) -> List:
	'''
	Flatten a list of lists or np.ndarrays without breaking strings into characters
	adapted from https://stackoverflow.com/questions/5286541/how-can-i-flatten-lists-without-splitting-strings
	
		params:
			l (list): a list to flatten
		
		returns:
			l (list): the list, flattened into 1 dimension
	'''
	l = apply_to_all_of_type(l, ListConfig, OmegaConf.to_container)
	if l is not None:
		return [k for j in ([i] if not isinstance(i,(list,tuple,np.ndarray)) else flatten(i) for i in l) for k in j]

def listify(l: 'any') -> List:
	'''
	If something isn't a list, return it as a list. If it is, return it without changing it.
		
		params:
			l (any)	: an object that we want to ensure is a list
		
		returns:
			l (list): the object as/in a list
	'''
	if isinstance(l,list):
		return l
	
	if isinstance(l,ListConfig):
		return OmegaConf.to_container(l)
	
	if isinstance(l,tuple):
		return list(l)
	
	if isinstance(l,(np.ndarray,pd.Series)):
		return l.tolist()
	
	return [l]

def unlistify(l: 'any') -> 'any':
	'''
	Safely unlists one-element lists/dicts
	
		params:
			l (any): an iterable object with 1 or more elements inside it
		
		returns:
			l (any): if iterable has only one object inside it, return that object.
					 otherwise, return the original object
	'''
	if isinstance(l,dict):
		return l[list(l.keys())[0]] if len(l) == 1 else l
	
	return l[0] if isinstance(l,list) or isinstance(l,tuple) and len(l) == 1 else l

def none(iterator: 'Iterator') -> bool:
	'''
	Convenience function wrapping not any/all not
	
		params:
			iterator: an iterator expression that generates booleans
		
		returns;
			bool 	: True if all expressions generated by iterator are False,
					  otherwise False
	'''
	return not any(iterator)

def multiplator(
	series: Union[List,pd.Series], 
	multstr: str = 'multiple'
) -> 'any':
	'''
	Get a single unique element from a list, or a default value if there are multiple elements
	
		params:
			series (list-like)	: a list with one or more unique values
			multstr (str)		: a string to return if the list has multiple elements
		
		returns:
			any 				: if there is 1 unique element in series, return that element
								  if there is >1 element in series, return multstr
								  if there is all of series is na or it is an empty series, return na
	'''
	# we cast to pd series here because it allows us to compare nans coming from an object array
	# numpy doesn't handle those correctly (it treats them as unique non-nan values or just gives up)
	series = pd.Series(series)
	
	if series.unique().size == 1:
		return series.unique()[0]
	elif not all(series.isna()) and series.unique().size > 1:
		return multstr
	else:
		return np.nan

def z_transform(x: np.ndarray) -> np.ndarray:
	'''
	Z-transform an array of numbers
		
		params:
			x (np.ndarray)	: an array with int/float values
		
		returns:
			np.ndarray 		: x expressed in standard devation units
	'''
	diffs = x - np.mean(x)
	return diffs/np.std(x)

def sem(x: Union[List,np.ndarray,torch.Tensor], na_rm: bool = True) -> float:
	'''
	Calculate the standard error of the mean for a list of numbers
	
		params:
			x (list) 		: a list of numbers for which to calculate the standard error of the mean
			na_rm (bool)	: exclude nas?
		
		returns:
			sem_x (float)	: the standard error of the mean of x
	'''
	namespace = torch if isinstance(x,torch.Tensor) else np
	if na_rm:
		x = [v for v in x if not namespace.isnan(v)]
		if namespace == torch:
			x = torch.tensor(x)
	
	if len(x) == 0:
		return namespace.nan
	else:	
		return namespace.std(x)/sqrt(len(x))

def set_seed(seed: int) -> None:
	'''
	Set all random seeds
	
		params:
			seed (int):	a number to set the random seeds to
	'''
	random.seed(seed)
	np.random.seed(seed)
	torch.manual_seed(seed)
	torch.cuda.manual_seed(seed)
	torch.cuda.manual_seed_all(seed)

def strip_punct(sentence: str) -> str:
	'''
	Removes (most) punctuation from a string
	
		params:
			sentence (str)	: a sentence to remove punctuation from
		
		returns:
			sentence (str)	: the sentence with punctuation removed
	'''
	return re.sub(r'[^\[\]\<\>\w\s,]', '', sentence)

def apply_to_all_of_type(
	data: 'any', 
	t: Type, 
	fun: Callable, 
	*args: Tuple, 
	**kwargs: Dict
) -> 'any':
	'''
	Apply a function to recursively to all elements in an iterable that match the specified type
		
		params:
			data (any)		: an object to recursively apply a function to
			t (type)		: the type of object within data to apply the function to
			fun (Callable)	: the function to apply to any values within data of type t
			*args (tuple)	: passed to fun
			**kwargs (dict)	: passed to fun
		
		returns:
			data (any)		: the data object with fun applied to everything in data matching type t
	'''
	if isinstance(data,(DictConfig,ListConfig)):
		# we need the primitive versions of these so we can modify them
		data = OmegaConf.to_container(data)
	
	data = deepcopy(data)
	
	if isinstance(data,t):
		returns = filter_none(fun(data, *args, **kwargs))
	elif isinstance(data,dict):
		returns = filter_none({apply_to_all_of_type(k, t, fun, *args, **kwargs): apply_to_all_of_type(v, t, fun, *args, **kwargs) for k, v in data.items()})
	elif isinstance(data,(list,tuple,set)):
		returns = filter_none(type(data)(apply_to_all_of_type(i, t, fun, *args, **kwargs) for i in data))
	elif isinstance(data,Dataset):
		# this means you have to cast the result back to a dataset afterward. I can't figure out how to do it inplace
		# since it can only be done from a dict but the dict is one level up
		returns = filter_none(list(apply_to_all_of_type(i, t, fun, *args, **kwargs) for i in data))
	elif isinstance(data,(torch.Tensor,pd.Series)):
		returns = filter_none(type(data)([apply_to_all_of_type(i, t, fun, *args, **kwargs) for i in data]))
	elif isinstance(data,np.ndarray):
		returns = filter_none(np.array([apply_to_all_of_type(i, t, fun, *args, **kwargs) for i in data]))
	else:
		returns = filter_none(data)
	
	if isinstance(data,(pd.Series,np.ndarray)):
		return returns if returns.any() or returns.size == 1 and returns[0] == 0 else None
	else:
		return returns

def filter_none(data: 'any') -> 'any':
	'''
	Remove None values recursively from iterables (i.e., [[None, None]] -> {nothing}, [[None, 'blah']] -> [['blah']])
	
		params:
			data (any)	: any data from which to remove None values
		
		returns:
			data (any)	: the data with None values removed
	'''
	# from https://stackoverflow.com/questions/20558699/python-how-recursively-remove-none-values-from-a-nested-data-structure-lists-a
	data = deepcopy(data)
	
	if isinstance(data,(list,tuple,set)):
		return type(data)(filter_none(x) for x in data if x is not None)
	elif isinstance(data,dict):
		return type(data)(
			(filter_none(k), filter_none(v))
				for k, v in data.items() if k is not None and v is not None
			)
	else:
		if isinstance(data,(pd.Series,np.ndarray)):
			return data if data.any() or data.size == 1 and data[0] == 0 else None
		else:
			return data

def recursor(
	t: 'type', 
	*args: Tuple, 
	**kwargs: Dict
) -> Callable:
	'''
	Creates recursive functions that apply to a single data type and do not rely on output from a previous step
		
		params:
			t (type)		: the type to apply the function to
			*args (tuple)	: passed to the called function
			**kwargs (dict)	: passed to the called function
	'''
	return lambda fun: \
		lambda data, *args, **kwargs: \
			apply_to_all_of_type(data=data, t=t, fun=fun, *args, **kwargs)


# summary and file related
def get_file_prefix(summary: pd.DataFrame) -> str:
	'''
	Creates an appropriate file prefix for saving various output files
	
		params:
			summary (pd.DataFrame)	: a summary dataframe containing information about the experiment's configuration
		
		returns:
			file_prefix (str)		: a string containing the dataset and epoch information to prefix to saved files
	'''
	dataset_name = summary.eval_data.unique()[0]
	epoch_label = multiplator(summary.epoch_criteria, multstr = '-')
	if summary.model_id.unique().size == 1 and not summary.model_id.unique()[0] == 'multiple':
		epoch_label = '-' + epoch_label
		magnitude = len(str(summary.total_epochs.unique()[0]))
		epoch_label = f'{str(summary.eval_epoch.unique()[0]).zfill(magnitude)}{epoch_label}'
		
	file_prefix = f'{dataset_name}-{epoch_label}'
	
	return file_prefix

def get_sentence_label(data: pd.DataFrame) -> str:
	'''
	Replaces masked tokens in sentences with display values for plot/accuracy labels
	
		params:
			data (pd.DataFrame)	: a dataframe containing sentence examples
		
		returns:
			sentence_ex (str)	: a sentence example from the data frame with mask tokens replaced with display values
	'''
	if not 'position_ratio_name' in data.columns:
		return data.sentence.unique()[0]
	
	first_rows = data[data.sentence == data.loc[0].sentence][['ratio_name', 'position_ratio_name', 'sentence']].drop_duplicates().reset_index(drop=True)
	
	position_map = {}
	for row in first_rows.index:
		position_map.update({gf: position for gf, position in tuple(zip(first_rows.loc[row].ratio_name.split('/'), [int(p) for p in first_rows.loc[row].position_ratio_name.replace('position ', '').split('/')]))})
	
	position_map = dict(sorted(position_map.items(), key=lambda item: item[1]))
	
	sentence_ex = first_rows.sentence[0]
	for gf in position_map:
		# this is bit hacky, but it's to ensure it'll work in cases with multiple models' data
		# where we can't rely on the mask token for each model being available.
		sentence_ex = re.sub(r'^(.*?)(\[MASK\]|\<mask\>)', f'\\1{gf}', sentence_ex)
	
	return sentence_ex

def get_data_for_pairwise_comparisons(
	summary: pd.DataFrame,
	eval_cfg: DictConfig = None, 
	cossims: bool = False, 
	diffs: bool = False
) -> List:
	'''
	Get data for performing pairwise comparisons for plots and accuracy metrics
	
		params:
			summary (pd.DataFrame)	: a summary containing results information
			eval_cfg (DictConfig)	: a configuration for the evaluation being performed
			cossims (bool)			: are you getting pairwise data for cosine similarities?
			diffs (bool)			: are you getting pairwise data for a differences measure?
		
		returns:
			summary (pd.DataFrame)	: the summary formatted for comparisons
			colnames (tuple[str])	: the column names corresponding to the summary metrics
			pairs (list)			: a list of pairs to compare data in colnames for
	'''
	
	def get_pairs(
		summary: pd.DataFrame, 
		eval_cfg: DictConfig = None, 
		cossims: bool = False
	) -> List[Tuple[str]]:
		'''
		Get pairs of values for comparison groups
		
			params:
				summary (pd.DataFrame)		: a summary containing results information
				eval_cfg (DictConfig)		: a configuration for the type of experiment being performed
				cossims (bool)				: are you getting pairs for cosine similarity measures?
			
			returns:
				pairs (list(tuple(str)))	: a list of pairs of strings to be used for comparisons
		'''
		if not cossims:
			# Get each unique pair of sentence types so we can create a separate plot for each pair
			types = summary.sentence_type.unique()
			types = sorted(types, key=lambda s_t: eval_cfg.data.sentence_types.index(s_t))
		else:
			types = summary.predicted_arg.unique()
		
		pairs = [pair for pair in list(itertools.combinations(types, 2)) if not pair[0] == pair[1]]
		
		if not cossims:
			# Sort so that the trained cases are first
			reference_sentence_type = multiplator(summary.reference_sentence_type)
			pairs = [sorted(pair, key=lambda x: str(-int(x == reference_sentence_type)) + x) for pair in pairs]
			
			# Filter to only cases including the reference sentence type for ease of interpretation
			pairs = [(s1, s2) for s1, s2 in pairs if s1 == reference_sentence_type] if reference_sentence_type != 'none' else pairs
		else:
			pairs = list(set(tuple(sorted(pair)) for pair in pairs))
		
		return pairs
	
	def format_summary_for_comparisons(
		summary: pd.DataFrame, 
		exp_type: str = None, 
		cossims: bool = False, 
		diffs: bool = False
	) -> Tuple[pd.DataFrame,Tuple[str]]:
		'''
		Formats a summary for comparisons by sorting for grammatical function prominence and returning the correct metrics colnames
		
			params:
				summary (pd.DataFrame)	: a summary to format
				exp_type (str)			: the type of experiment being evaluated
				cossims (bool)			: are you making comparisons for cosine similarities?
				diffs (bool)			: are you making comparisons for a difference measure?
			
			returns:
				summary (pd.DataFrame)	: the summary sorted according to grammatical function prominence (if applicable)
				colnames (tuple)		: the colnames in summary containing the evaluation metrics
		'''
		summary = summary.copy()
				
		if exp_type == 'newverb' and not cossims:
			# Sort by grammatical function prominence for newverb exps (we do this because 'subj' alphabetically follows 'obj'), but it's more natural for it to go first
			# if the ratio_name's first element doesn't exist in the GF_ORDER dict, then just sort by it normally (as in newverb sentences data)
			summary['ratio_order'] 	= [GF_ORDER.index(gf_ratio_name.split('/')[0]) if gf_ratio_name.split('/')[0] in GF_ORDER else gf_ratio_name for gf_ratio_name in summary.ratio_name]
			summary 				= summary.sort_values(['model_id', 'ratio_order'])
			summary 				= summary.drop('ratio_order', axis=1)
			summary['ratio_name'] 	= [re.sub(r'\[|\]', '', ratio_name) for ratio_name in summary.ratio_name]
		
		colnames 	= get_eval_metrics_colnames(exp_type, cossims, diffs)
		metric 		= colnames[-2]
		
		if summary.model_id.unique().size > 1:
			colnames[-2] = f'{metric}_mean'
		else:
			summary[f'{metric}_sem'] = 0
		
		return summary, colnames
	
	def get_eval_metrics_colnames(
		exp_type: str, 
		cossims: bool = False, 
		diffs: bool = False
	) -> List[str]:
		'''
		Get the correct metric for the type of experiment being performed
		
			params:
				exp_type (str)	: the type of experiment being performed
				cossims (bool)	: are you getting colnames for cosine similarity measures?
				diffs (bool)	: are you getting colnames for a differences measure?
			
			returns:
				colnames (list)	: a list of strings containing the correct column name of the metric and sem measures for the given experiment type/data
		'''
		if not cossims:
			metric = 'odds_ratio' if exp_type == 'newarg' or not diffs else 'odds_ratio_pre_post_difference' if exp_type == 'newverb' else None
		elif cossims:
			metric = 'cossim'
		
		semmetric = f'{metric}_sem'
		
		return [metric, semmetric]
	
	exp_type 			= eval_cfg.data.exp_type if eval_cfg is not None else None
	summary, colnames 	= format_summary_for_comparisons(summary, exp_type, cossims=cossims, diffs=diffs)
	pairs 				= get_pairs(summary, eval_cfg, cossims)
	
	return summary, tuple(colnames), pairs

def transfer_hyperparameters_to_df(
	source: pd.DataFrame, 
	target: pd.DataFrame
) -> pd.DataFrame:
	'''
	Transfers hyperparameter information from one dataframe to another
	
		params:
			source (pd.DataFrame) : a dataframe containing hyperparameter information
			target (pd.DataFrame) : a dataframe not containing hyperparameter information
		
		returns:
			target (pd.DataFrame) : target with the hyperparameters information from source added
	'''
	hp_cols = [
		c for c in source.columns if not c in [
			'odds_ratio', 'odds_ratio_mean', 'odds_ratio_sem', 'ratio_name', 'position_ratio_name',
			'log_probability', 'other_log_probability',
			'role_position', 'token_type', 'token_id', 'token', 
			'sentence', 'sentence_type', 'sentence_num', 'odds_ratio_pre_post_difference',
			'odds_ratio_pre_post_difference_mean', 'odds_ratio_pre_post_difference_sem',
			'both_correct', 'both_incorrect', 'gen_correct', 'gen_incorrect', 
			'ref_correct', 'ref_incorrect', 'ref_correct_gen_incorrect',
			'ref_incorrect_gen_correct', 'specificity', 'specificity_se',
			'gen_given_ref', 's1', 's2', 's1_ex', 's2_ex', 'arg_type', 'other_arg_type',
			'predicted_arg', 'predicted_role'
		] 
		and not c.endswith('_ref') and not c.endswith('_gen')
	]
	
	for c in hp_cols:
		target[c] = multiplator(source[c])
	
	return target

def get_single_pair_data(
	summary: pd.DataFrame, 
	pair: Tuple[str], 
	group: str, 
	pair_col: str = 'sentence_type'
) -> Tuple:
	'''
	Get data for a single pairwise comparison
	
		params:
			summary (pd.DataFrame)			: a dataframe containing experiment results
			pair (tuple(str))				: a tuple with labels indicating the pair to get data for
			group (str)						: a column name indicating groups to compare data for 
											  (comparison data must include information about all groups)
			pair_col (str)					: which column contains the labels in pair?
		
		returns:
			x_data, y_data (pd.DataFrame)	: information from summary where pair_col == pair[0] and pair_col == pair[1]
											  and all groups in the group column occur in both x and y
	'''
	# do some sorting
	summary = summary[summary[pair_col].isin(pair)].reset_index(drop=True)
	
	# we need 'kind='stable'' because otherwise other columns get into weird orders
	summary = summary.sort_values(group, kind='stable').reset_index(drop=True)
	
	if 'ratio_name' in summary and any(re.sub(r'\[|\]', '', gf) in ratio_name for gf in GF_ORDER for ratio_name in summary.ratio_name):
		# Sort by grammatical function prominence for newverb exps (we do this because 'subj' alphabetically follows 'obj'), but it's more natural for it to go first
		summary['ratio_order'] 	= [GF_ORDER.index(f'[{gf_ratio_name.split("/")[0]}]') for gf_ratio_name in summary.ratio_name]
		summary 				= summary.sort_values(['model_id', 'ratio_order'])
		summary 				= summary.drop('ratio_order', axis=1)
		summary.ratio_name		= [re.sub(r'\[|\]', '', ratio_name) for ratio_name in summary.ratio_name]
	
	x_data = summary[summary[pair_col] == pair[0]].reset_index(drop=True)
	y_data = summary[summary[pair_col] == pair[1]].reset_index(drop=True)
	
	# Filter data to groups that only exist in both sets
	common_groups = set(x_data[group]).intersection(y_data[group])
	x_data = x_data[x_data[group].isin(common_groups)].reset_index(drop=True)
	y_data = y_data[y_data[group].isin(common_groups)].reset_index(drop=True)
	
	return x_data, y_data

def get_accuracy_measures(
	refs: pd.DataFrame, 
	gens: pd.DataFrame, 
	colname: str
) -> Dict:
	'''
	Get prediction accuracy measures
	
		params:
			refs (pd.DataFrame)	: a dataframe containing measures from the reference group
			gens (pd.DataFrame)	: a dataframe containing measures from the generalization group
			colname (str)		: the name of the column containing the values to compare
		
		returns:
			dict 				: a dictionary containing accuracy information about refs and gens
								  where accuracy is computed based on colname > 0 (intended for use with odds ratios)
	'''
	refs 						= refs.copy().reset_index(drop=True)
	gens 						= gens.copy().reset_index(drop=True)
	
	# need to convert to float for scipy stats
	# we could do this with torch, but given the output is harder to work with it's not really worth it
	r, p_r						= pearsonr([float(t) for t in refs[colname]], [float(t) for t in gens[colname]])
	
	refs_correct 				= refs[colname] > 0
	gens_correct 				= gens[colname] > 0
	num_points 					= len(refs.index)
	
	gen_given_ref 				= sum(gens_correct.loc[np.where(refs_correct)])/len(refs_correct.loc[np.where(refs_correct)]) * 100 if not refs_correct.loc[np.where(refs_correct)].empty else np.nan
	both_correct 				= sum(refs_correct * gens_correct)/num_points * 100
	both_incorrect				= sum(-refs_correct * -gens_correct)/num_points * 100
	ref_correct 				= sum(refs_correct)/num_points * 100
	ref_incorrect 				= 100. - ref_correct
	gen_correct 				= sum(gens_correct)/num_points * 100
	gen_incorrect 				= 100. - gen_correct
	ref_correct_gen_incorrect 	= sum( refs_correct * -gens_correct)/num_points * 100
	ref_incorrect_gen_correct 	= sum(-refs_correct *  gens_correct)/num_points * 100
	
	sq_err						= (gens[colname] - refs[colname])**2
	specificity 				= np.mean(sq_err)
	specificity_se 				= np.std(sq_err)/sqrt(num_points)
	
	return {
		'gen_given_ref'				: gen_given_ref,
		'both_correct'				: both_correct,
		'both_incorrect'			: both_incorrect,
		'ref_correct'				: ref_correct,
		'ref_incorrect'				: ref_incorrect,
		'gen_correct'				: gen_correct,
		'gen_incorrect'				: gen_incorrect,
		'ref_correct_gen_incorrect'	: ref_correct_gen_incorrect,
		'ref_incorrect_gen_correct'	: ref_incorrect_gen_correct,
		'specificity_(MSE)'			: specificity,
		'specificity_se'			: specificity_se,
		'r'		 					: r,
		'p_r'						: p_r,		
	}

def get_odds_ratios_accuracies(
	summary: pd.DataFrame, 
	eval_cfg: DictConfig, 
	get_diffs_accuracies: bool = False
) -> pd.DataFrame:
	'''
	Create a dataframe containing information about odds ratios accuracies
	
		params:
			summary (pd.DataFrame)			: a summary containing odds ratios results
			eval_cfg (DictConfig)			: the evaluation's parameters
			get_diffs_accuracies (bool)		: are you getting accuracy for improvements in odds ratios over the experiment?
		
		returns:
			acc (pd.DataFrame)				: a dataframe containing accuracy information
	'''
	def update_acc(acc: List[Dict], refs: pd.DataFrame, gens: pd.DataFrame, colname: str, **addl_columns) -> None:
		'''
		Update the accuracy list
		
			params:
				acc (List)				: the list of accuracy measures to update
				refs (pd.DataFrame)		: the data from the reference group
				gens (pd.DataFrame)		: the data from the generalization group
				colname (str)			: the name of the column containing the measure to use to compute accuracy
				**addl_columns (dict)	: additional information to include in each added dictionary
		'''
		acc_data = get_accuracy_measures(refs=refs, gens=gens, colname=colname)
		
		cols = {
			**addl_columns,
			**acc_data
		}
		
		if 'token' in refs.columns and 'token_id' in refs.columns:
			cols.update({
				'token'		: multiplator(refs.token, multstr='any'),
				'token_id'	: multiplator(refs.token_id),
			})
		
		if 'token_type' in refs.columns:
			cols.update({**cols, 'token_type': multiplator(refs.token_type)})
		
		acc.append(cols)
	
	summary, (odds_ratio, odds_ratio_sem), paired_sentence_types = get_data_for_pairwise_comparisons(summary, eval_cfg=eval_cfg, diffs=get_diffs_accuracies)
	
	acc = []
	for pair in paired_sentence_types:
		x_data, y_data = get_single_pair_data(summary, pair, 'ratio_name')
		
		s1_ex = get_sentence_label(x_data)
		s2_ex = get_sentence_label(y_data)
		
		common_args = {
			 'acc'						: acc,
			 'colname'					: odds_ratio,
			 's1'						: pair[0], 
			 's2'						: pair[1], 
			 's1_ex'					: s1_ex, 
			 's2_ex'					: s2_ex,
			 'ratio_name'				: multiplator(x_data.ratio_name),
			 'predicted_arg'			: multiplator(x_data.ratio_name.str.split('/')[0], multstr='any'),
		}
		
		if 'position_ratio_name' in x_data.columns and 'position_ratio_name' in y_data.columns:
			common_args.update({
				f'position_ratio_name_ref': multiplator(x_data.position_ratio_name),
				f'position_ratio_name_gen': multiplator(y_data.position_ratio_name),
			})
		
		if eval_cfg.data.exp_type == 'newverb':
			common_args.update({'args_group': multiplator(x_data.args_group, multstr='any')})
		elif eval_cfg.data.exp_type == 'newarg':
			common_args.update({'predicted_role': multiplator(x_data.role_position, multstr='any')})
		
		update_acc(refs=x_data, gens=y_data, **common_args)
		
		if x_data.ratio_name.unique().size > 1:
			for name, x_group in x_data.groupby('ratio_name'):
				y_group = y_data[y_data.ratio_name == name]
				
				common_args.update({
					 'ratio_name'				: name,
					 'predicted_arg'			: name.split('/')[0]
				})
				
				if 'position_ratio_name' in x_group.columns and 'position_ratio_name' in y_group.columns:
					common_args.update({
						f'position_ratio_name_ref'	: multiplator(x_group.position_ratio_name),
						f'position_ratio_name_gen'	: multiplator(y_group.position_ratio_name),
					})
				
				if eval_cfg.data.exp_type == 'newarg':
					common_args.update({'predicted_role': x_group.role_position.unique()[0].split()[0]})
				
				update_acc(refs=x_group, gens=y_group, **common_args)
				
				if 'token' in x_group.columns and x_group.token.unique().size > 1:
					for token, x_token_group in x_group.groupby('token'):
						y_token_group = y_data[y_data.token == token]
						update_acc(refs=x_token_group, gens=y_token_group, **common_args)
	
	acc = pd.DataFrame(acc)
	
	return acc

def move_cols(
	df: pd.DataFrame, 
	cols_to_move: List[str] = [], 
	ref_col: str = None, 
	position: str = 'after'
) -> pd.DataFrame:
    '''
    Reorders columns in a dataframe by name and (optionally) relative position to a reference column.
    From https://towardsdatascience.com/reordering-pandas-dataframe-columns-thumbs-down-on-standard-solutions-1ff0bc2941d5
    
    	params:
    		df (pd.DataFrame)	: a dataframe
    		cols_to_move (list)	: a list of column names to be moved
    		ref_col (str)		: the column relative to which the cols to move should be positioned
    		place (str)			: one of 'before', 'after'. whether to place the cols to move before or after the ref col
    	
    	returns:
    		df (pd.DataFrame)	: the dataframe with the columns reordered
    '''
    cols 		= list(df.columns)
    cols_to_move = listify(cols_to_move)
    
    # if the ref col is not provide or is not in the columns, move columns to the front
    index 		= cols.index(ref_col) if ref_col is not None and ref_col in cols else 0
    position 	= 'before' if index == 0 else position
           
    if position == 'after':
        seg1 = cols[:index+1]
        seg2 = cols_to_move
    else:
        seg1 = cols[:index]
        seg2 = cols_to_move + ([ref_col] if ref_col else [])
    
    seg1 = [i for i in seg1 if i not in seg2]
    seg3 = [i for i in cols if i not in seg1 + seg2]
    
    return(df[seg1 + seg2 + seg3])


# Tokenizer utilities
def create_tokenizer_with_added_tokens(
	model_id: str, 
	tokens_to_mask: List[str], 
	delete_tmp_vocab_files: bool = True, 
	**kwargs
) -> 'PreTrainedTokenizer':
	'''
	Creates a tokenizer with tokens added to be just like new words
	
		params:
			model_id (str)					: the huggingface model name of the tokenizer
			tokens_to_mask (list)			: a list containing the tokens to add to the model vocabulary
			delete_tmp_vocab_files (bool)	: whether to delete the temporary vocabulary files created while making the tokenizer
			**kwargs (dict)					: additional arguments passed to transformers.AutoTokenizer.from_pretrained
		
		returns:
			tokenizer (pretrainedtokenizer)	: a tokenizer with the tokens in tokens_to_mask added to the vocabulary in the *right* way
	'''
	kwargs.update(dict(use_fast=False))
	
	if re.search(r'(^(distil)?bert-)|(multiberts-)', model_id):
		return create_bert_tokenizer_with_added_tokens(model_id, tokens_to_mask, delete_tmp_vocab_files, **kwargs)
	elif re.search(r'^roberta-', model_id):
		return create_roberta_tokenizer_with_added_tokens(model_id, tokens_to_mask, delete_tmp_vocab_files, **kwargs)	
	else:
		raise ValueError('Only BERT, DistilBERT, and RoBERTa tokenizers are currently supported.')

def create_bert_tokenizer_with_added_tokens(
	model_id: str, 
	tokens_to_mask: List[str], 
	delete_tmp_vocab_files: bool = True, 
	**kwargs
) -> Union['BertTokenizer', 'DistilBertTokenizer']:
	'''
	Create a BERT/DistilBERT tokenizer with added tokens correctly
	
		params:
			model_id (str)					: the huggingface model name of the tokenizer to create
			tokens_to_mask (list)			: the tokens to add to the tokenizer's vocabulary
			delete_tmp_vocab_files (bool)	: whether to delete the temporary vocab files used to create the tokenizer
			**kwargs (dict)					: passed to transformers.AutoTokenizer.from_pretrained
		
		returns:
			tokenizer (berttokenizer)		: a BertTokenizer with the tokens_to_mask added to the vocabulary
	'''
	
	if 'uncased' in model_id or 'multiberts' in model_id:
		tokens_to_mask = [t.lower() for t in tokens_to_mask]
	
	bert_tokenizer = AutoTokenizer.from_pretrained(model_id, **kwargs)
	vocab = bert_tokenizer.get_vocab()
	assert not verify_tokens_exist(bert_tokenizer, tokens_to_mask), f'New token(s) already exist(s) in {model_id} tokenizer!'
	
	for token in tokens_to_mask:
		vocab.update({token: len(vocab)})
	
	with open('vocab.tmp', 'w', encoding='utf-8') as tmp_vocab_file:
		tmp_vocab_file.write('\n'.join(vocab))
	
	exec(f'from transformers import {bert_tokenizer.__class__.__name__}')
	
	tokenizer = eval(bert_tokenizer.__class__.__name__)(name_or_path=model_id, vocab_file='vocab.tmp', **kwargs)
	
	# for some reason, we have to re-add the [MASK] token to bert to get this to work, otherwise
	# it breaks it apart into separate tokens '[', 'mask', and ']' when loading the vocab locally (???)
	# this does not affect the embedding or the model's ability to recognize it
	# as a mask token (e.g., if you create a filler object with this tokenizer, 
	# it will identify the mask token position correctly)
	tokenizer.add_tokens(tokenizer.mask_token, special_tokens=True)
	tokenizer.model_max_length = bert_tokenizer.model_max_length
	
	if delete_tmp_vocab_files:
		os.remove('vocab.tmp')
	
	assert verify_tokens_exist(tokenizer, tokens_to_mask)
	
	return tokenizer

def create_roberta_tokenizer_with_added_tokens(
	model_id: str, 
	tokens_to_mask: List[str], 
	delete_tmp_vocab_files: bool = True, 
	**kwargs
) -> 'RobertaTokenizer':
	'''
	Creates a RobertaTokenizer with new tokens added in a way that doesn't break tokenization of everything else
	
		params:
			model_id (str)					: the huggingface model name of the tokenizer
			tokens_to_mask (list)			: the list of tokens to add to the vocabulary
			delete_tmp_vocab_files (bool)	: whether to delete the temporary vocab and merges files used to create the tokenizer
			**kwargs (dict)					: passed to transformers.AutoTokenizer.from_pretrained
		
		returns:
			tokenizer (RobertaTokenizer)	: a RobertaTokenizer with the tokens in tokens_to_mask added in a way that doesn't
											  break existing tokenizations and works correctly
	'''
	if 'uncased' in model_id or 'multiberts' in model_id:
		tokens_to_mask = [t.lower() for t in tokens_to_mask]
	
	roberta_tokenizer = AutoTokenizer.from_pretrained(model_id, **kwargs)
	assert not verify_tokens_exist(roberta_tokenizer, tokens_to_mask), f'New token {token} already exists in {model_id} tokenizer!'
	
	vocab = roberta_tokenizer.get_vocab()
	
	for token in tokens_to_mask:
		vocab.update({token: len(vocab)})
	
	with open('vocab.tmp', 'w', encoding = 'utf8') as tmp_vocab_file:
		json.dump(vocab, tmp_vocab_file, ensure_ascii=False)
	
	merges = [' '.join(key) for key in roberta_tokenizer.bpe_ranks.keys()]
	# we have to add a newline at the beginning of the file
	# since it's expecting it to be a comment, so we add a blank string here
	# that will get joined with a newline
	merges = [''] + get_roberta_bpes_for_new_tokens(tokens_to_mask) + merges
	merges = list(dict.fromkeys(merges)) # drops any duplicates we may have happened to add while preserving order
	with open('merges.tmp', 'w', encoding = 'utf-8') as tmp_merges_file:
		tmp_merges_file.write('\n'.join(merges))
	
	exec(f'from transformers import {roberta_tokenizer.__class__.__name__}')
	
	tokenizer = eval(roberta_tokenizer.__class__.__name__)(name_or_path=model_id, vocab_file='vocab.tmp', merges_file='merges.tmp', **kwargs)
	
	# for some reason, we have to re-add the <mask> token to roberta to get this to work, otherwise
	# it breaks it apart into separate tokens '<', 'mask', and '>' when loading the vocab and merges locally (???)
	# this does not affect the embeddings or the model's ability to recognize it
	# as a mask token (e.g., if you create a filler object with this tokenizer, 
	# it will identify the mask token position correctly)
	tokenizer.add_tokens(tokenizer.mask_token, special_tokens=True)
	tokenizer.model_max_length = roberta_tokenizer.model_max_length
	
	if delete_tmp_vocab_files:
		os.remove('vocab.tmp')
		os.remove('merges.tmp')
	
	assert verify_tokens_exist(tokenizer, tokens_to_mask), 'New tokens were not added correctly!'
	
	return tokenizer

def get_roberta_bpes_for_new_tokens(new_tokens: List[str]) -> List[str]:
	'''
	Get the B(yte)-P(air) (E)ncodings for tokens to add to a roberta tokenizer
	
		params:
			new_tokens (list)	: the list of new tokens to generate BPEs for
		
		returns;
			bpes (list)			: the list of BPEs (to be used to create a new robertatokenizer with the tokens in new_tokens tokenized correctly as single tokens)
	'''
	def gen_roberta_bpes(new_token: str, highest: bool = True) -> List[str]:
		'''
		Generate roberta BPEs for a single new token (abandon all hope, ye who enter here...)
		
			params:
				new_token (str)	: the token to generate BPEs for
				highest (bool)	: whether the function was called from inside (False) or outside (True)
			
			returns:
				bpes (list)		: a list of the BPEs needed to add to the beginning of roberta's merges file
								  to ensure that the new token is tokenized as a single token
		'''
		# I do not completely understand how this works, but it does
		# a lot of trial and error is recorded here...
		chrs = [c for c in new_token]
		if len(chrs) == 2:
			if not highest:
				return tuple([chrs[0], chrs[1]])
			else:
				return [' '.join([chrs[0], chrs[1]])]
		
		if len(chrs) == 3:
			if not highest:
				return tuple([chrs[0], ''.join(chrs[1:])])
			else:
				return gen_roberta_bpes(chrs[1:]) + [' '.join([chrs[0], ''.join(chrs[1:])])]
		
		if len(chrs) % 2 == 0:
			pairs = gen_roberta_bpes(''.join(chrs[:-2]), highest = False)
			pairs += gen_roberta_bpes(''.join(chrs[-2:]), highest = False)
			pairs += tuple([''.join(chrs[:-2]), ''.join(chrs[-2:])])
			if not highest:
				return pairs
		else:
			pairs = gen_roberta_bpes(''.join(chrs[:-3]), highest = False)
			pairs += gen_roberta_bpes(''.join(chrs[-2:]), highest = False)
			pairs += gen_roberta_bpes(''.join(chrs[-3:]), highest = False)
			pairs += tuple([''.join(chrs[:-3]), ''.join(chrs[-3:])])
			if not highest:		
				return pairs
		
		pairs = tuple(zip(pairs[::2], pairs[1::2]))
		pairs = [' '.join(pair) for pair in pairs]
		# sp = chr(288)
		
		# pairs with the preceding special token
		# g_pairs = []
		# for pair in pairs:
		# 	if re.search(r'^' + ''.join(pair.split(' ')), new_token):
		# 		g_pairs.append(chr(288) + pair)
		
		# pairs = g_pairs + pairs
		# pairs = [f'{sp} {new_token[0]}'] + pairs
		
		pairs = list(dict.fromkeys(pairs)) # remove any duplicates
		
		return pairs
	
	roberta_bpes = [gen_roberta_bpes(new_token) for new_token in new_tokens]
	roberta_bpes = [pair for token in roberta_bpes for pair in token]
	return roberta_bpes

@recursor(str)
def format_roberta_tokens_for_tokenizer(data: str) -> str:
	'''
	Format a roberta token for use with a robertatokenizer given an input in display formatting
	Recursor means that this applies recursively to any nested data structure, formatting all tokens for use with roberta,
	and outputs data in the same shape as the input
		
		params:
			data (str)	: a token formatted for display to format for use with the tokenizer
		
		returns:
			data (str)	: the token formatted for use with a roberta tokenizer
	'''
	if not data.startswith(chr(288)) and not data.startswith('^'):
		data = f'{chr(288)}{data}'
	elif data.startswith('^'):
		data = re.sub(r'^\^', '', data)
	
	return data

@recursor(str)
def format_roberta_tokens_for_display(token: str) -> str:
	'''
	Format a roberta token for display purposes (summary files and plots)
	Recursor means that this applies recursively to any nested data structure, formatting all tokens for use with roberta,
	and outputs data in the same shape as the input
	
		params:
			token (str): the token (in tokenizer format) to format for display
		
		returns:
			token (str): the token formatted for display
	'''
	if token.startswith(chr(288)):
		token = re.sub(rf'^{chr(288)}', '', token)
	elif not token.startswith('^'):
		token = f'^{token}'
	
	return token
	
@recursor(str)
def format_strings_with_tokens_for_display(
	data: str, 
	tokenizer_tokens: List[str],
	tokens_to_uppercase: List[str], 
	model_name: str, 
	string_id: str
) -> str:
	'''
	Format a string with tokens in it for display
	Recursor means that this applies recursively to any nested data structure, formatting all tokens for use with roberta,
	and outputs data in the same shape as the input
	
		params:
			data (str)					: a string containing tokens in tokenizer format to format for display
			tokenizer_tokens (list) 	: the list of potential tokenizer tokens in data to format (other words will be left alone)
			tokens_to_uppercase (list)	: which tokens to make uppercase (we want added tokens, but not existing tokens in tokenizer_tokens to be made uppercase)
			model_name (str)			: the model we are formatting tokens from
			string_id (str)				: the huggingface string id for the model
		
		returns:
			data (str)					: the data with tokens in tokenizer_tokens formatted for display
	'''
	for token in listify(tokenizer_tokens):
		if model_name == 'roberta':
			if token.startswith(chr(288)) or token.startswith('^'):
				token = token[1:]
			
			data = re.sub(rf'(?<!{chr(288)}){token}', f'^{token}', data)
			data = re.sub(rf'{chr(288)}{token}', token, data)
		
		# this might need to be adjusted if we ever use an uncased roberta model,
		# since we'll need to check after modifying the token to token[1:] above
		elif ('uncased' in string_id or 'multiberts' in string_id) and token in tokens_to_uppercase:
			data = re.sub(token, token.upper(), data)
	
	return data

@recursor(str)
def format_data_for_tokenizer(
	data: str, 
	mask_token: str, 
	string_id: str, 
	remove_punct: bool
) -> str:
	'''
	Format a string for use with a tokenizer
	Recursor means that this applies recursively to any nested data structure, formatting all tokens,
	and outputs data in the same shape as the input
	
		params:
			data (str)			: the data to format for use with a tokenizer
			mask_token (str)	: the tokenizer's mask token
			string_id (str)		: the huggingface string id for the tokenizer
			remove_punct (bool)	: whether to remove punctuation from the string
		
		returns:
			data (str)			: the data formatted for use with the tokenizer in string_id
	'''
	data = data.lower() if 'uncased' in string_id or 'multiberts' in string_id else data
	data = strip_punct(data) if remove_punct else data
	data = data.replace(mask_token.lower(), mask_token)
	return data

def verify_tokens_exist(
	tokenizer: 'PreTrainedTokenizer', 
	tokens: Union[List[str],List[int]]
) -> bool:
	'''
	Verify that tokens exist as single tokens in a tokenizer's vocabulary
	
		params:
			tokenizer (PreTrainedTokenizer)	: a huggingface tokenizer
			tokens (list)					: a list of token strings or ids to check
		
		returns:
			bool 							: True iff the tokens in tokens are tokenized as single non unknown tokens by tokenizer, False otherwise
	'''
	token_ids = filter_none([
		tokenizer.convert_tokens_to_ids(token) 
		if isinstance(token,str) 
		else token if isinstance(token,int) 
		else None for token in listify(tokens)
	])
	
	return none(token_id == tokenizer.convert_tokens_to_ids(tokenizer.unk_token) for token_id in token_ids)

def verify_tokenization_of_sentences(
	tokenizer: 'PreTrainedTokenizer', 
	sentences: List[str], 
	tokens_to_mask: List[str] = None,
	**kwargs: Dict
) -> bool:
	'''
	Verify that a custom tokenizer and one created using from_pretrained behave identically except on the tokens to mask
	
		params:
			tokenizer (PreTrainedTokenizer)	: the custom tokenizer to compare against a pretrained baseline
			sentences (list)				: the sentences to compare
			tokens_to_mask (list)			: the tokens that exist in tokenizer but not the pretrained baseline to ignore for comparison purposes
			**kwargs (dict)					: passed to transformers.AutoTokenizer.from_pretrained
		
		returns:
			True if the passed tokenizer and a pretrained default one treat all sentences identically except on the tokens to mask, false otherwise
	'''
	tokens_to_mask = deepcopy(tokens_to_mask)
	assert verify_tokens_exist(tokenizer, tokens_to_mask), f'Tokens in {tokens_to_mask} were not correctly added to the tokenizer!'
	
	kwargs.update(dict(use_fast=False))
	
	tokenizer_id 	= tokenizer.name_or_path
	tokenizer_one 	= tokenizer
	tokenizer_two 	= AutoTokenizer.from_pretrained(tokenizer_id, **kwargs)
	sentences 		= flatten(sentences)
	
	if 'roberta' in tokenizer.name_or_path:
		# to replace the tokens in the sentences for comparison with roberta, we need to get the display versions
		# we can't do this on the tokenized sentences, because those will break apart the new tokens in different ways
		tokens_to_mask = format_roberta_tokens_for_display(tokens_to_mask)
		
	if tokens_to_mask:
		masked_sentences = []
		for sentence in sentences:
			for token in tokens_to_mask:
				sentence = sentence.replace(token, tokenizer.mask_token)
			
			masked_sentences.append(sentence)
	
	# we compare things by determining identity on the sentences with the new tokens replaced with mask tokens
	# we cannot do this by comparing the ids directly, since how surrounding material gets tokenized depends on whether there is a mask token present
	tokenizations_one = tokenizer_one(masked_sentences, return_tensors='pt', padding=True)['input_ids']
	tokenizations_two = tokenizer_two(masked_sentences, return_tensors='pt', padding=True)['input_ids']
	identical = torch.all(torch.eq(tokenizations_one, tokenizations_two))
	
	return identical

def reinitialize_token_weights(
	word_embeddings: nn.Embedding,
	tokens_to_initialize: List[str],
	tokenizer: 'PreTrainedTokenizer',
	device: str = 'cpu',
	seed: int = None,
) -> Tuple[nn.Embedding, int]:
	'''
	Reinitializes token weights to random values to provide variability in model tuning
	
		params:
			word_embeddings (nn.Embedding)	: the word embeddings to reinitialize
			tokens_to_initialize (list)		: which tokens to reinitialize the embeddings of
			tokenizer (pretrainedtokenizer)	: used to convert tokens to ids
			device (str)					: which device to put the new embeddings on
			seed (int)						: the random seed to use when generating new embedding weights
		
		returns:
			word_embeddings (nn.Embedding)	: the embeddings with the weights of the specified tokens
											  reinitialized to random values
			seed (int)						: if no seed was provided, the random seed used to generate the tokens
											  is returned as well
	'''
	assert verify_tokens_exist(tokenizer, tokens_to_initialize), 'Tokens do not exist in the vocabulary!'
	
	return_seed = False
	
	with torch.no_grad():
		model_embedding_dim 	= word_embeddings.shape[-1]
		num_new_tokens 			= len(tokens_to_initialize)
		new_embeds 				= nn.Embedding(num_new_tokens, model_embedding_dim).to(device)
		
		std, mean 				= torch.std_mean(word_embeddings)
		log.info(f'Initializing {num_new_tokens} new token(s) with random data drawn from N({mean:.2f}, {std:.2f})')
		
		# we do this here manually because otherwise running multiple models using multirun was giving identical results
		# we set this right before initializing the weights for reproducability
		if seed is None:
			seed 		= int(torch.randint(2**32-1, (1,)))
			return_seed = True
		
		set_seed(seed)
		log.info(f'Seed set to {seed}')
		
		nn.init.normal_(new_embeds.weight, mean=mean, std=std)
		
		for i, token in enumerate(tokens_to_initialize):
			token_id 					= tokenizer.convert_tokens_to_ids(token)
			word_embeddings[token_id] 	= new_embeds.weight[i]
	
	if return_seed:
		return word_embeddings, seed
	else:
		return word_embeddings

def load_format_dataset(
	dataset_loc: str,
	split: str,
	data_field: str,
	string_id: str,
	n_examples: int = None,
	remove_punct: bool = False,
	tokenizer_kwargs: Dict = {},
) -> Dataset:
	'''
	Loads and formats a huggingface dataset according to the passed options
	
		params:
			dataset_loc (str)		: the location of the dataset
			split (str)				: the split to load from the dataset
			data_field (str)		: the field in the dataset that contains the actual examples
			string_id (str)			: a string_id corresponding to a huggingface pretrained tokenizer
									  used to determine appropriate formatting
			n_examples (int)		: how many (random) examples to draw from the dataset
									  if not set, the full dataset is returned
			fmt (str)				: the file format the dataset is saved in
			remove_punct (bool)		: whether to remove punctuation from formatted sentences
			tokenizer_kwargs (dict)	: passed to AutoTokenizer.from_pretrained
		
		returns:
			dataset (Dataset)		: a dataset that has been formatted for use with the tokenizer,
									  possible with punctuation stripped
	'''
	dataset 				= DatasetDict.load_from_disk(dataset_loc)[split]
	
	# gather these so we know the position of items prior to shuffling 
	# and we can figure out what to convert to torch.Tensors later
	original_cols 			= list(dataset.features.keys())
	original_pos			= list(range(dataset.num_rows))
	
	# cast to dict since datasets cannot be directly modified
	dataset 				= dataset.to_dict()
	dataset['original_pos']	= original_pos
	original_cols.append('original_pos')
	
	baseline_tokenizer 		= AutoTokenizer.from_pretrained(string_id, **tokenizer_kwargs)
	mask_token 				= baseline_tokenizer.mask_token
	dataset[data_field]		= format_data_for_tokenizer(data=dataset[data_field], mask_token=mask_token, string_id=string_id, remove_punct=remove_punct)
	
	# now cast back to a dataset
	dataset 				= Dataset.from_dict(dataset)
	dataset 				= sample_from_dataset(dataset, n_examples)
	dataset 				= dataset.map(lambda ex: baseline_tokenizer(ex[data_field]))
	
	dataset.set_format(type='torch', columns=[f for f in dataset.features if not f in original_cols])
	
	return dataset

def sample_from_dataset(
	dataset: Union[Dataset,Dict], 
	n_examples: int = None, 
	log_message: bool = True
) -> Dataset:
	'''
	Draws n_examples random examples from a dataset.
	Does not shuffle if the number of examples >= the length of the dataset
	
		params:
			dataset (Dataset)	: the dataset to draw examples from
			n_examples (int)	: how many examples to draw
			log_message (bool)	: whether to display a message logging the number of examples drawn
								  compared to the size of the full dataset
		
		returns:
			dataset (Dataset)	: a dataset with n_examples random selections from the passed dataset
	'''
	n_examples	= dataset.num_rows if n_examples is None else min(n_examples, dataset.num_rows)
	
	if n_examples < dataset.num_rows:
		if log_message:
			log.info(f'Drawing {n_examples} random examples from {len(dataset)} total')
		
		dataset = dataset.shuffle().select(range(n_examples))
	
	return dataset

def mask_input(
	inputs: torch.Tensor,
	tokenizer: 'PreTrainedTokenizer',
	indices: List[int] = None, 
	masking_style: str = 'always',
	device: str = 'cpu',
) -> torch.Tensor:
	'''
	Creates a masked input according to the specified masking style.
	If indices are provided, mask them according to the masking style.
	Otherwise, choose a random 15% of the indices to mask.
	
	Then:
		If masking is "none", return the original tensor.
		If masking is "always", replace indices with mask tokens.
		If masking is "bert", Replace 80% of indices with mask tokens, 
							  10% with the original token, 
							  and 10% with a random token.
	
		params:
			inputs (torch.Tensor)			: a tensor containing token ids for a model
			tokenizer (PreTrainedTokenizer)	: the tokenizer for the model for which the inputs are being prepared
			indices (list[int])				: which positions to mask in the input
											  if no indices are passed and masking is in ['bert', 'roberta', 'none'],
											  a random 15% of tokens will be chosen to mask
			masking_style (str)				: a string specifying which masked tuning style to use
											  'always' means always replace the indices with the mask
											  'none' means leave the original indices in place
											  'bert/roberta' means replace the indices with the mask 80% of the time,
											  	with the original token 10% of the time,
											  	and with a random token id 10% of the time
			device (str)					: what device (cpu, cuda) to place any returned indices on
		
		returns:
			masked_inputs (torch.Tensor)	: inputs where the indices have been replaced with the mask token id
											  according to the masked tuning style
			indices (List[int])				: if no indices were passed, a list of the randomly chosen indices 
											  to mask is returned 
	'''
	return_indices = False
	if indices is None:
		return_indices = True
		# exclude the pad tokens
		candidates 	= (inputs != tokenizer.convert_tokens_to_ids(tokenizer.pad_token)).nonzero(as_tuple=True)[0]
		indices 	= torch.argsort(torch.rand(candidates.shape[0], device=device))[:round(candidates.shape[0]*.15)]
		# this is just for presentational purposes really
		indices 	= indices.sort().values
	
	# if we are not masking anything just return it
	if masking_style == 'none':
		if return_indices:
			return inputs, candidates
		else:
			return inputs
	
	masked_inputs 	= inputs.clone().detach()	
	mask_token_id 	= tokenizer.convert_tokens_to_ids(tokenizer.mask_token)
	
	for index in indices:
		# even when using bert/roberta style tuning, we sometimes need access to the data with everything masked
		# do not use bert/roberta-style masking if we are always masking
		# note that we DO want to allow collecting unmasked inputs even when using always masked tuning, since we need them for the labels
		# setting this to 0 means we always mask if masking_style is none
		r = np.random.random() if masking_style in ['bert', 'roberta'] else 0
			
		# Roberta tuning regimen: 
		# masked tokens are masked 80% of the time,
		# original 10% of the time, 
		# and random word 10% of the time
		if r < 0.8:
			replacement = mask_token_id
		elif 0.8 <= r < 0.9:
			replacement = inputs[index]
		elif 0.9 <= r:
			replacement = np.random.choice(len(tokenizer))
		
		masked_inputs[index] = replacement
	
	if return_indices:
		return masked_inputs, indices
	else:
		return masked_inputs


# analysis
def get_best_epoch(
	loss_df: pd.DataFrame, 
	method: str = 'mean'
) -> int:
	'''
	Determine the best performance achieved during training
		
		params:
			loss_df (pd.DataFrame)	: a dataframe containing information about loss performance
			method (str)			: which method to use to determine best performance. one of {best_mean, best_sumsq}
		
		returns:
			epoch (int)				: the epoch on which the best performance, according to method, was achieved on dev sets
	'''
	loss_df = loss_df.copy().sort_values(['dataset', 'epoch']).reset_index(drop=True)
	log_message = True
	
	if method in ['best_sumsq', 'sumsq']:
		best_losses = loss_df.loc[loss_df.groupby('dataset').value.idxmin()].reset_index(drop=True)
		
		epoch_sumsqs = []
		for dataset, df in best_losses.groupby('dataset'):
			dataset_epoch = df.epoch.unique()[0]
			epoch_losses = loss_df[loss_df.epoch == dataset_epoch].value
			epoch_avg_loss = epoch_losses.mean()
			sumsq = sum((epoch_avg_loss - epoch_losses)**2)
			epoch_sumsqs.append([dataset_epoch, sumsq])
		
		epoch_sumsqs = sorted(epoch_sumsqs, key=lambda epoch_sumsq: epoch_sumsq[1])
		best_epoch = epoch_sumsqs[0][0]
		log.info(f'Best epoch is {best_epoch} (sumsq loss = ' + '{:.2f}'.format(epoch_sumsqs[0][1]) + f', minimum for {", ".join(best_losses[best_losses.epoch == best_epoch].dataset.values)}.')
		
	elif method in ['best_mean', 'mean']:
		mean_losses = loss_df.groupby('epoch').value.agg('mean')
		lowest_loss = mean_losses.min()
		best_epoch = mean_losses.idxmin()
		log.info(f'Best epoch is {best_epoch} (mean loss = ' + '{:.2f}'.format(lowest_loss) + ').')
	else:
		best_epoch = loss_df.epoch.max()
		log.warning(f'No method for determining best epoch provided (use "sumsq", "mean"). The highest epoch {best_epoch} will be used. This may not be the best epoch!')
		# this is so we don't print out the log message below if the best epoch isn't actually the highest one
		log_message = False
	
	if best_epoch == max(loss_df.epoch) and log_message:
		log.warning('Note that the best epoch is the final epoch. This may indicate underfitting.')
	
	return best_epoch

def load_csvs(
	csvs: List[str], 
	**kwargs: Dict
) -> pd.DataFrame:
	'''
	Combines csvs into a single dataframe
	
		params:
			csvs (list)		: a list of paths to csv files to combine
			**kwargs (dict)	: passed to pd.read_csv
		
		returns:
			pd.DataFrame 	: a dataframe concatenating all the csvs in csvs
	'''
	csvs = [csvs] if isinstance(csvs, str) else csvs
	return pd.concat([pd.read_csv(f, **kwargs) for f in tqdm(csvs)], ignore_index=True)

def load_pkls(pkls: List[str]) -> pd.DataFrame:
	'''
	Combines pickled dataframes into a single dataframe
	
		params:
			pkls (list)		: a list of paths to dataframe pkl files to combine
		
		returns:
			pd.DataFrame 	: a dataframe concatenating all the pickled dataframes in pkls
	'''
	pkls = [pkls] if isinstance(pkls, str) else pkls
	return pd.concat([pd.read_pickle(f) for f in tqdm(pkls)], ignore_index=True)

def ungzip(files: List[str]) -> None:
	'''
	Extract several gzipped files. Used with the R analysis script since it's much quicker to do this in Python
	
		params:
			files (list): a list of gzipped files to extract
	'''
	dests = [re.sub(r'\.gz$', '', f) for f in files]
	fs_dests = tuple(zip(files, dests))
	for f, dest in tqdm(fs_dests):
		with gzip.open(f, 'rb') as f_in, open(dest, 'wb') as f_out:
			copyfileobj(f_in, f_out)

def delete_files(files: List[str]) -> None:
	'''
	Delete several files. Used with the R analysis script since it's much quicker to do this in Python
	
		params:
			files (list): a list of files to delete
	'''
	for f in tqdm(files):
		try: 
			os.remove(f)
		except:
			print(f'Unable to remove "{f}".')
			continue

class gen_summary():
	'''
	Generates summary statistics for saved dataframes, allowing R-style unquoted arguments to functions
	for convenience, via manipulation of globals(). There is a failsafe in case any variables are
	already defined in globals, which will still allow the use of quoted strings.
	'''
	def __init__(self, 
		df: Union[pd.DataFrame,str,'TextIOWrapper','StringIO'], 
		columns: List = ['gen given ref', 'correct', 'odds ratio pre-post difference', 'odds ratio', 'cossim'],
		funs: List = ['mean', 'sem']
	) -> 'gen_summary':
		'''
		Create a gen_summary instance
		
			params:
				df 				: a pandas dataframe, str, or file-like object
				columns 		: a list of columns to generate summary statistics for that are in df.columns
				funs 			: a list of summary statistics to generate using pandas' agg function
			
			returns:
				gen_summary 	: a summary generator that can be called with (un)quoted column names
								  to generate a summary of columns in df
		'''
		# allow passing a filepath
		if not isinstance(df, pd.DataFrame):
			df = pd.read_csv(df)
			
		self.df 		= df
		self.columns 	= [c for c in columns if c in self.df.columns]
		self.funs 		= funs
		self.prep_quasiquotation()
	
	def __call__(self, *columns: Tuple) -> pd.DataFrame.groupby:
		'''
		Generate a summary using the columns specified, print and return it
			
			params:
				*columns (tuple)				: a list of unquote names of columns in summary to group summary statistics by
			
			returns:
				results (pd.DataFrame.groupby) 	: a summary consisting of self.funs applied to the columns in self.columns
												  grouped by the passed columns
		'''
		agg_dict = {c : self.funs for c in self.columns if c in self.df.columns}
		results = self.df.groupby(list(columns)).agg(agg_dict).sort_values(list(columns))
		print(results)
		return results
	
	def prep_quasiquotation(self) -> None:
		'''
		Adds column names to global variables to facilitate R-like quasiquotation usage.
		As a failsafe, if a variable is already defined with a conflicting definition,
		we return early without adding any names. In this case, quoted strings may be provided to generate a summary.
		'''
		for c in self.df.columns:
			if c in globals() and not globals()[c] == c:
				print(f"{c} already has a conflicting definition: '{globals()[c]}'. Exiting.")
				return
		
		for c in self.df.columns:
			globals()[c] = c
	
	def set_funs(self, *funs: Tuple) -> None:
		'''
		Set the summary functions to use for this gen_summary
			
			params: 
				funs (tuple): 	a tuple of functions known to pandas by string 
								name used to generate summary statistics
		'''
		self.funs = list(funs)
	
	def set_columns(*columns: Tuple) -> None:
		'''
		Set the numerical columns to get summary statistics for
			
			params:
				columns (tuple): a tuple of columns in self.df to generate summary statistics for
		'''
		self.columns = [c for c in list(columns) if c in self.df.columns]
	
	def set_df(self, df: Union[pd.DataFrame,str,'TextIOWrapper','StringIO']) -> None:
		'''
		Set the df to a different one, and attempt to reinitialize all variables
			
			params:
				df (pd.DataFrame,str,IO):	a dataframe object, filehandler, or path to a csv
		'''
		if not isinstance(df, pd.DataFrame):
			df = pd.read_csv(df)
		
		self.df = df
		
		# reset the columns to remove any that aren't present in the new df
		set_columns(self.df.columns)
		
		# reinitialize quasiquotation variables using the columns from the new df
		# note that if the new df uses column names already defined in the previous df
		# this will throw an error, but some unquoted string names may still work
		self.prep_quasiquotation()