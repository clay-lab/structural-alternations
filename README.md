# structural-alternations

Examining whether pre-trained language models have understanding of structural alternations


## Installation

Dependencies are managed using `conda`. To set up the conda environment for the framework, issue the following command from within the `structural-alternations` directory.
```bash
conda env create -f environment.yaml
```
Once the environment has been created, activate with `conda activate salts`.

## Usage

There are three main types of experiments which can be run using the `structural-alternations` framework: **non-tuning**, **tuning**, and **new verb** experiments. 

**Non-tuning** experiments involve taking an off-the-shelf pre-trained model and examining its logit distributions on masked language modeling (MLM) tasks for a variety of pre-determined tokens or token groups, allowing you to examine things like entropy or token-group confidence in particular positions in testing data for pre-trained BERT models. 

**Tuning** experiments allow you to take a small set of tuning data and fine-tune a pre-trained model by introducing nonce words into the model's vocabulary. You can then test how the model performs on MLM tasks vis-a-vis its predictions on how these nonce tokens are used. You can set `masked_tuning_style` to either `always` (default) or `bert`. `bert` style uses the masked tuning method from the original BERT specification (tokens that are masked are replaced with masked tokens 80% of the time, with the target 10% of the time, and with a random token in the model's vocabulary 10% of the time). If tuning a RoBERTa model, the `bert` option does not work, so it is automatically set to `always`. NOTE THAT IN THIS CASE, THE INCORRECT TUNING SETTING IS REFLECTED IN THE DIRECTORY NAME AND CONFIG FILES, SO BE CAREFUL!

**New verb** experiments are a combination of the non-tuning and tuning experiments described above. In this case, the model is tuned on a nonce word that is a verb that occurs with various kinds of arguments that are assigned to specific grammatical functions (e.g., tuning data might encode that the word "laughter" is a possible subject but not a possible object for the nonce verb, while "tree" has the opposite distribution). However, it is not directly tuned on the arguments of that verb themselves (the embeddings are frozen). Then, you can examine what the model has learned about the new verb by examining its predictions regarding those non-tuned arguments in various structures pre- and post-tuning.

**get_args.py** is a utility to help in running new verb experiments. Part of the key to these experiments is that the arguments associated with each position should be unbiased toward those positions prior to tuning, since that means that whatever knowledge the model ends up with about the positions of the arguments with the nonce verb must have come from the tuning and not the training. For this reason, it is useful to generate random nouns as candidates, and check how well they adhere to this ideal. To do this, you will need the SUBTLEX corpus file `SUBTLEX-US frequency list with PoS information.xlsx`. Note that some code in the file is commented out---you should uncomment this the first time you run it. It is recommended that you set a breakpoint after SUBTLEX is read in and manually save the file to CSV at that point, to facilitate faster loading on subsequent runs. The script will filter out nouns based on a `min_freq`, and also based on whether they exist within all model tokenizers as a single token. Then, a random set of `tuning.num_words` * `tuning.args` nouns is generated. You may set `interactive=true` (the default) to manually approve a set of nouns for evaluation, or you may specify `n_sets` to automatically generate `n_sets` when `interactive=False`. (`interactive=True` overrides `n_sets`, so you can stop whenever you are ready.) To do this, the predictions each model makes for the combinations of every noun with every other noun are collected and converted to surprisal values. The raw results are stored in `predictions.csv` and `predictions.pkl`. Then, summary statistics, including a difference of differences measure is calculated. In an ideal case, this would be 0, which would indicate that the expected argument is not considered by the pre-fine-tuned models to be more likely in the expected position than the unexpected argument. The difference of these measures is also reported, which gives an idea of how much more biased the models are toward putting the expected noun in the expected position vs. in the unexpected position. Positive means that it expects the first type of noun in the expected position more than the other type; negative means the opposite. The lower this difference of differences gets post-tuning, the more equally the model has learned about which nouns go where.

You can also set whether to `strip_punct`uation from the sentences used as tuning and test data (default: `false`). Note that any punctuation used to signal masked tokens will not be stripped from anywhere in the sentence. Punctuation that will not be stripped is `[]<>,`. In addition, commas cannot be used in evaluation data, as they are used to separate sentences of different types. Note that currently, models can only be evaluated on data that matches the data they were trained on for this setting (e.g., if a model was trained with no punctuation, it will be evaluated on data with no punctuation, and vice versa). Keep this in mind when using `multieval.py`, as the individual models being summarized across may be evaluated on slightly different eval data if they were trained differently in this way. In case a reported summary statistic comes from models with multiple different parameters, the value for that parameter will be reported as `multi`, with no direct information about which underlying parameters generated the summary statistic retained in the full summary.

### Framework Interface

In order to tune a new model, run `python tune.py`. To override the default choices for `model` type and `tuning` data, specify them as `key=value` arguments to the tuning script, such as:
```bash
python tune.py model=bert tuning=untuned
```
This script will pull a vanilla BERT model from HuggingFace, do nothing with it (i.e., tune it on an empty dataset), and save the randomly initialized weights of any tokens marked as to be masked to the outputs directory. Note that the valid `key`s for a loaded script represent folders within the `conf/` directory and the `value`s are the names of the YAML configuration files within those directories (without the `.yaml` file extension).

Note that if you want to tune or evaluate models with multiple sets of parameters, it is highly recommended you install the `hydra-joblib-launcher` module using `pip install hydra-joblib-launcher --upgrade`. Then, when running models using hydra's `--multirun/-m` flag, you can set `hydra/launcher=joblib` and `hydra.launcher.n_jobs=#`, where `#` is replaced with the number of concurrent jobs you want to run. This greatly speeds things up.

When you actually tune a model, a file containing the weights of each token in the tuning configuration's `to_mask` list for each epoch (including the randomly initialized weights) will be saved in the outputs directory. In addition, a CSV and pickle file containing various training metrics for each epoch with be saved in the outputs directory, along with a PDF containing plots of these metrics over time. These metrics include the loss and the log probability for each masked token. For new verb experiments, these metrics also report a difference measure, which is the difference for each argument position between the mean surprisal of the unexpected tokens in that position and the mean surprisal of the expected tokens in that position. This is expected to increase as the model learns about which position each set of arguments goes in.

In order to evaluate a single model's performance, run `python eval.py`. To override the default choices for `checkpoint_dir` and evaluation `data`, specify them as `key=value` arguments to the tuning script, as above. This outputs a CSV and a pickle file containing a dataframe that reports the evaluation results in `checkpoint_dir/eval`, as well as plots for each pairwise combination of sentence types in the data file. You may also specify which epoch you want to evaluate the performance of the tuned model on by setting the `epoch` parameter (the default evaluates the final state of each model). The script will pull the weights from the corresponding epoch in the saved weights file.

In order to evaluate multiple models' performance, run `python multieval.py`. You may set `criteria` for which models to evaluate as a string consisting of a comma separated list of strings (make sure to include the initial and trailing quotes `'` when setting this, or hydra will try to parse them as separate parameters). To be included in the evaluation, the directory name containing the model must be in a subdirectory of `checkpoint_dir` and must contain all criteria strings. Note that to facilitate not outputting the evaluation results within several subdirectories, the character `^` can be used instead of the system path separator to specify that a given string should occur at the beginning of a subdirectory. This is replaced with the system path separator when the script is run. On Windows, you need to escape this character in the criteria string with an additional `^` (e.g., `^^bert`) so that Windows doesn't parse it out before passing it to hydra. If no criteria are set, all models in every subdir of the set `checkpoint_dir` will be evaluated on the specified evaluation `data`, and results will be saved in the appropriate subdirectory (if there is no evaluation data already present for that model). Once all models in every subdir of `checkpoint_dir` have been evaluated, the results are combined and reduced to a single point (plus or minus the standard error) for each model. As before, this outputs a CSV and a pickle file containing the summary, as well as a PDF with plots that report this information, in `checkpoint_dir/multieval-[criteria]-[data]`.

### Directory Structure

`structural-alternations` uses the `hydra` framework to encapsulate experiment
parameters and hyperparameters in modular YAML files for repoducibility. All
configuration files are stored within the `conf/` directory:

  - `conf/tune.yaml` controls the hyperparameters for a tuning experiment. You must provide a `model` type (default: `distilbert`) and a source of tuning data (default `dative_DO_give_active`), and you may optionally override hyperparameters like the `max_epochs` and `min_epochs` for tuning (default: `70`), the `patience` (default = `max_epochs`) and a `delta` (default = `0`). `delta` specifies how much of an improvement is required when determining patience; `0` means any improvement on the average dev set loss, no matter how small, resets the patience counter. You can also set the masking style for the tuning inputs (default: `always` masked), and the learning rate `lr` of the model during tuning (default: `0.001`). Finally, you can specify a `dev` set to compute metrics on during tuning, which is a file structured just like a tuning data file (i.e., any tuning data may be used as a dev set, and vice versa). You may also use `dev=best_matches` to automatically include all tuning data that differ from the tuning data in one dimension (e.g., lexical choice, voice, etc.) in the dev sets as determined by splitting the name of the tuning files by underscores and comparing the number of matching strings in the lists. `dev_exclude` may be used to manually specify a list of strings that serve as exclusion criteria for dev sets. For instance, if you want to include all data that differ in one dimension from `dative_DO_give_active` except for passives, you could set `dev_exclude=[passive]`. In all cases, the masked tuning data is used as a dev set with dropout disabled in addition to any other dev sets.
  - `conf/eval.yaml` controls the `data` used for model evaluation (default: `ptb_active_dbl_obj`) and the relative directory path `checkpoint_dir` of the model used for evaluation (no default provided; these will appear in the `outputs/` directory once a model has been downloaded and optionally tuned). You may also specific which `epoch` you want to evaluate the tuned model on.
  - `conf/tuning/` is a directory containing configuration files for tuning a model. Each file must specify four things: a `name`, the `reference_sentence_type` (i.e., what kind of sentence(s) do the tuning data represent), whether the tuning data is for a `new_verb` experiment, a dictionary of tokens `to_mask` of the form `"short_name" : "model_vocab_idx"` which will be masked out during tuning, and a list of tuning `data` sentences. If you are running a new verb experiment, you must also provide `args`, a dictionary of the form `str : List[str]`, where the final dataset for tuning will be constructed from the `data` by generating replacing `str` in every sentence in `data` with every possible value in the list.
  - `conf/model/` is a directory containing configuration files for the types of pre-trained models. Each file must specify the `base_class`, the `tokenizer` class, and the `string_id` of a pre-trained model family from HuggingFace.
  - `conf/data/` is a directory containing configuration files for the various evaluation datasets. Each file must specify five things: a unique `name`, pointing to a CSV file in the `data/` directory (note, the project-root-level subdirectory, not the configuration directory) containing the actual evaluation data, a short `description` of the dataset, whether or not the entries in the data file represent `entail`ment relations (i.e., whether during evaluation it must be the case that success on the second sentence in a loaded line is predicated by success on the first sentence), whether or not the data is for a `new_verb` experiment, a list of `sentence_types` in the order they occur in the data file, a dictionary of `eval_groups` of the form `"group_name" : ["list", "of", "tokens"]` specifying groups of tokens used by the evaluation scripts to compute model performance on the dataset, and a dictionary of tokens `to_mask` from the evaluation data.